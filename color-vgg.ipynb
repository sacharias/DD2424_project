{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, time\n",
    "# from IPython.display import Image, display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import lab2rgb, rgb2lab, rgb2gray\n",
    "from skimage import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data as D\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import glob\n",
    "import os.path as osp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Den här är helt komplett och fungerar*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'vgg-mse-'\n",
    "test = False\n",
    "custom_loss = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation\n",
    "os.makedirs('images/train/class/', exist_ok=True)\n",
    "os.makedirs('images/val/class/', exist_ok=True)\n",
    "for i, file in enumerate(os.listdir('testSet_resize')):\n",
    "    if i < 1000:\n",
    "        os.rename('testSet_resize/' + file, 'images/val/class/' + file) # 1k images\n",
    "    else:\n",
    "        os.rename('testSet_resize/'+ file, 'images/train/class/' + file) # 40k images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlacesImages(D.Dataset):\n",
    "    def __init__(self, root, transform):\n",
    "        self.filenames = []\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        \n",
    "        for fn in glob.glob(osp.join(self.root, '*.jpg')):\n",
    "            self.filenames.append(fn)\n",
    "        \n",
    "        self.len = len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.filenames[index])\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "\n",
    "        img = self.transform(img)\n",
    "        img_original = np.asarray(img)\n",
    "\n",
    "        img_lab = rgb2lab(img_original)\n",
    "        img_lab = (img_lab + 128) / 255\n",
    "        img_ab = img_lab[:, :, 1:3]\n",
    "        img_ab = torch.from_numpy(img_ab.transpose((2,0,1))).float()\n",
    "        img_original = rgb2gray(img_original)\n",
    "        img_original = torch.from_numpy(img_original).unsqueeze(0).float()\n",
    "        return img_original, img_ab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test:\n",
    "    batch_size = 4\n",
    "    train_path = 'images/sub_train/class'\n",
    "    val_path = 'images/sub_val/class'\n",
    "else:\n",
    "    batch_size = 64\n",
    "    train_path = 'images/train/class'\n",
    "    val_path = 'images/val/class'   \n",
    "    \n",
    "train_transforms = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip()])\n",
    "train_images = PlacesImages(train_path, train_transforms)\n",
    "train_loader = D.DataLoader(train_images, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "val_transforms = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224)])\n",
    "val_images = PlacesImages(val_path, val_transforms)\n",
    "val_loader = D.DataLoader(val_images, batch_size=batch_size, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.Upsample(scale_factor=4)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.net1 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            \n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),  \n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size = 3, stride = 2, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size = 3, stride = 2, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 2, padding = 2, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 2, padding = 2, dilation = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, stride = 2, padding = 2, dilation = 1),\n",
    "            nn.ReLU(),      \n",
    "        )\n",
    "        self.net2 = nn.Sequential(\n",
    "            nn.Linear(4608, 6272),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6272, 6272),\n",
    "        )\n",
    "        self.upsample = nn.Upsample(scale_factor=4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.net1(x)\n",
    "        x = self.net2(x.view(x.size(0), -1))\n",
    "        x = self.upsample(x.view(x.size(0), 2, 56, 56))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGGNet(\n",
       "  (net1): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (19): ReLU()\n",
       "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
       "    (21): ReLU()\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
       "    (23): ReLU()\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
       "    (25): ReLU()\n",
       "  )\n",
       "  (net2): Sequential(\n",
       "    (0): Linear(in_features=4608, out_features=6272, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=6272, out_features=6272, bias=True)\n",
       "  )\n",
       "  (upsample): Upsample(scale_factor=4.0, mode=nearest)\n",
       ")"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VGGNet()\n",
    "model.to(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_loss\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.005\n",
    "def customLoss():\n",
    "    def criterion(trained, target):\n",
    "        MSE = torch.mean((trained - target) ** 2)\n",
    "        return (MSE + alpha / trained.std()), MSE\n",
    "    return criterion\n",
    "\n",
    "\n",
    "if custom_loss:\n",
    "    criterion = customLoss()\n",
    "    print('custom_loss')\n",
    "else:\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataiter = iter(train_loader)\n",
    "#feats, labels = dataiter.next()\n",
    "#print(feats.shape)\n",
    "#print(labels.shape)\n",
    "#outputs = model(feats)\n",
    "#print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVWriter(object):\n",
    "    def __init__(self):\n",
    "        self.csv_file = open(model_name + 'stats.csv', 'a')\n",
    "        self.csv_file.write('epoch, train_mse, val_mse, train_loss, val_loss, train_std, val_std \\n')\n",
    "        self.csv_file.close()\n",
    "        \n",
    "    def write(self, epoch, train_mse, val_mse, train_loss, val_loss, train_std, val_std):\n",
    "        self.csv_file = open(model_name + 'stats.csv', 'a')\n",
    "        self.csv_file.write('{}, {}, {}, {}, {}, {}, {} \\n'.format(epoch, train_mse, val_mse, train_loss, val_loss, train_std, val_std))\n",
    "        self.csv_file.close()\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def calc_std(output_ab):\n",
    "    ab = output_ab.detach().cpu().numpy()\n",
    "    return np.std(ab)\n",
    "\n",
    "def to_rgb(gray_input, ab_input, save_path=None, save_name=None, save_gray=True):\n",
    "    plt.clf()\n",
    "    color_img = torch.cat((gray_input, ab_input), 0).numpy()\n",
    "    color_img = color_img.transpose((1, 2, 0))\n",
    "    color_img[:, :, 0:1] = color_img[:, :, 0:1] * 100\n",
    "    color_img[:, :, 1:3] = color_img[:, :, 1:3] * 255 - 128\n",
    "    color_img = lab2rgb(color_img.astype(np.float64))\n",
    "    gray_input = gray_input.squeeze().numpy()\n",
    "    \n",
    "    plt.imsave(arr=color_img, fname=\"{}{}\".format(save_path['colorized'], save_name))\n",
    "    if save_gray:\n",
    "        plt.imsave(arr=gray_input, fname=\"{}{}\".format(save_path['grayscale'], save_name), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    print(\"Starting epoch {}\".format(epoch))\n",
    "    model.train()\n",
    "    batch_time, losses, mse, std_dev = AverageMeter(), AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    \n",
    "    end = time.time()\n",
    "    for i, (input_gray, input_ab) in enumerate(train_loader):\n",
    "        input_gray, input_ab = input_gray.to(dev), input_ab.to(dev)\n",
    "        \n",
    "        output_ab = model(input_gray)\n",
    "        if custom_loss:\n",
    "            loss, MSE_val = criterion(output_ab, input_ab)\n",
    "            losses.update(loss.item(), input_gray.size(0))\n",
    "            mse.update(MSE_val.item(), input_gray.size(0))\n",
    "        else:\n",
    "            loss = criterion(output_ab, input_ab)\n",
    "            losses.update(loss.item(), input_gray.size(0))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end) # record time for forward + backward\n",
    "        end = time.time()\n",
    "        \n",
    "        std_dev.update(calc_std(output_ab)) # record std_dev\n",
    "        \n",
    "        if i % 25 == 0:\n",
    "            print('Epoch [{0}][{1}/{2}]\\tTime {3:.3f}\\tLoss {4:.4f}'.format(epoch, i, len(train_loader), batch_time.avg, losses.avg))\n",
    "    \n",
    "    return losses.avg, std_dev.avg, mse.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch, save_all=False):\n",
    "    print(\"Start validation\")\n",
    "    model.eval()\n",
    "    batch_time, losses, mse, std_dev = AverageMeter(), AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    \n",
    "    end = time.time()\n",
    "    already_saved_images = False\n",
    "    for i, (input_gray, input_ab) in enumerate(val_loader):\n",
    "        input_gray, input_ab = input_gray.to(dev), input_ab.to(dev)\n",
    "        \n",
    "        output_ab = model(input_gray)\n",
    "        if custom_loss:\n",
    "            loss, MSE_val = criterion(output_ab, input_ab)\n",
    "            losses.update(loss.item(), input_gray.size(0))\n",
    "            mse.update(MSE_val.item(), input_gray.size(0))\n",
    "        else:\n",
    "            loss = criterion(output_ab, input_ab)\n",
    "            losses.update(loss.item(), input_gray.size(0))\n",
    "        \n",
    "        std_dev.update(calc_std(output_ab)) # record std_dev\n",
    "        \n",
    "        if not already_saved_images and not save_all:\n",
    "            already_saved_images = True\n",
    "            for j in range(min(len(output_ab), 10)):\n",
    "                save_path = {'grayscale': model_name + 'outputs/gray/', 'colorized': model_name + 'outputs/color/'}\n",
    "                save_name = 'img-{}-epoch-{}.jpg'.format(i * val_loader.batch_size + j, epoch)\n",
    "                to_rgb(input_gray[j].cpu(), ab_input=output_ab[j].detach().cpu(),\n",
    "                       save_path=save_path, save_name=save_name, save_gray=not(epoch > 0))\n",
    "        \n",
    "        if save_all:\n",
    "            print('len', len(output_ab))\n",
    "            print(output_ab.size())\n",
    "            for j in range(len(output_ab)):\n",
    "                save_path = {'grayscale': model_name + 'validation/gray/', 'colorized': model_name + 'validation/color/'}\n",
    "                save_name = 'img-{}.jpg'.format(i * val_loader.batch_size + j)\n",
    "                to_rgb(input_gray[j].cpu(), ab_input=output_ab[j].detach().cpu(), save_path=save_path, save_name=save_name)\n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "    \n",
    "    return losses.avg, std_dev.avg, mse.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(model_name + 'outputs/color', exist_ok=True)\n",
    "os.makedirs(model_name + 'outputs/gray', exist_ok=True)\n",
    "os.makedirs(model_name + 'checkpoints', exist_ok=True)\n",
    "best_losses = 1e10\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Epoch [0][0/101]\tTime 1.496\tLoss 0.5623\n",
      "Epoch [0][25/101]\tTime 0.738\tLoss 16073180.0236\n",
      "Epoch [0][50/101]\tTime 0.720\tLoss 8194185.5381\n",
      "Epoch [0][75/101]\tTime 0.797\tLoss 5498729.9110\n",
      "Epoch [0][100/101]\tTime 0.823\tLoss 4168613.2249\n",
      "Start validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sacharias/anaconda3/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 2939 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/Users/sacharias/anaconda3/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 383 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/Users/sacharias/anaconda3/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 3058 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/Users/sacharias/anaconda3/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 628 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Epoch [1][0/101]\tTime 3.251\tLoss 0.0543\n",
      "Epoch [1][25/101]\tTime 0.963\tLoss 0.0357\n",
      "Epoch [1][50/101]\tTime 0.917\tLoss 0.0332\n",
      "Epoch [1][75/101]\tTime 0.920\tLoss 0.0326\n",
      "Epoch [1][100/101]\tTime 0.915\tLoss 0.7874\n",
      "Start validation\n",
      "Starting epoch 2\n",
      "Epoch [2][0/101]\tTime 2.824\tLoss 0.0716\n",
      "Epoch [2][25/101]\tTime 1.013\tLoss 58.8018\n",
      "Epoch [2][50/101]\tTime 0.960\tLoss 30.0170\n",
      "Epoch [2][75/101]\tTime 0.971\tLoss 20.1541\n",
      "Epoch [2][100/101]\tTime 0.955\tLoss 27.3896\n",
      "Start validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sacharias/anaconda3/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 26048 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 3\n",
      "Epoch [3][0/101]\tTime 2.646\tLoss 6.2794\n",
      "Epoch [3][25/101]\tTime 0.979\tLoss 0.8560\n",
      "Epoch [3][50/101]\tTime 0.937\tLoss 0.4835\n",
      "Epoch [3][75/101]\tTime 0.918\tLoss 17.2436\n",
      "Epoch [3][100/101]\tTime 0.913\tLoss 13.0881\n",
      "Start validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sacharias/anaconda3/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 1012 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/Users/sacharias/anaconda3/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 668 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/Users/sacharias/anaconda3/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 3166 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/Users/sacharias/anaconda3/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 4072 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_f = CSVWriter()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_std, train_mse = train(train_loader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss, val_std, val_mse = validate(val_loader, model, criterion, epoch)\n",
    "    \n",
    "    csv_f.write(epoch, train_mse, val_mse, train_loss, val_loss, train_std, val_std)\n",
    "    \n",
    "    if val_loss < best_losses:\n",
    "        best_losses = val_loss\n",
    "        torch.save(model.state_dict(), model_name + 'checkpoints/model-epoch-{}-losses-{:.3f}.pth'.format(epoch+1, val_loss))\n",
    "\n",
    "torch.save(model.state_dict(), model_name + 'checkpoints/model-epoch-{}-losses-{:.3f}.pth'.format(epochs, val_loss))\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGGNet(\n",
       "  (net1): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (14): ReLU()\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU()\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (19): ReLU()\n",
       "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
       "    (21): ReLU()\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
       "    (23): ReLU()\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
       "    (25): ReLU()\n",
       "  )\n",
       "  (net2): Sequential(\n",
       "    (0): Linear(in_features=4608, out_features=6272, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=6272, out_features=6272, bias=True)\n",
       "  )\n",
       "  (upsample): Upsample(scale_factor=4.0, mode=nearest)\n",
       ")"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pretrained = torch.load('vgg-mse-checkpoints/model-epoch-3-losses-0.037.pth')\n",
    "#model.load_state_dict(pretrained)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start validation\n",
      "len 4\n",
      "torch.Size([4, 2, 224, 224])\n",
      "len 4\n",
      "torch.Size([4, 2, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sacharias/anaconda3/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 3547 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/Users/sacharias/anaconda3/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 175 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/Users/sacharias/anaconda3/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 2086 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len 3\n",
      "torch.Size([3, 2, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sacharias/anaconda3/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 3315 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/Users/sacharias/anaconda3/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 2216 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "/Users/sacharias/anaconda3/lib/python3.7/site-packages/skimage/color/colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 2781 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.makedirs(model_name + 'validation/color', exist_ok=True)\n",
    "os.makedirs(model_name + 'validation/gray', exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    validate(val_loader, model, criterion, 0, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
